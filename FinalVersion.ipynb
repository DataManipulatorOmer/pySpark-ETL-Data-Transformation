{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-0zyld9B4FR"
      },
      "source": [
        "#Pyspark On Collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mimn2kwQCnvp"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiMZnDskDGuu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMsXZplJDQyS"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "#for MaxRows\n",
        "#spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', True)\n",
        "#To Enable the Spark Rows Show\n",
        "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
        "#spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgVJJnjwB0Wr"
      },
      "source": [
        "#Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDXXxEihDXOO"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from functools import reduce\n",
        "from operator import add\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import DataFrame\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdi-lXoEX-No"
      },
      "source": [
        "#CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpV1SQrdZNUH"
      },
      "source": [
        "We will take the source columns and target columns from here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRG8msNWYAZS",
        "outputId": "666d873b-da7a-44d1-e813-ea0c1cb53746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------+----------------+\n",
            "| ID|SourceCol|       TargetCol|\n",
            "+---+---------+----------------+\n",
            "|  1|     Name|    CustomerName|\n",
            "|  2| PostCode|CustomerPostCode|\n",
            "|  3|      DOB|     CustomerDOB|\n",
            "|  4|      AGE|     CustomerAge|\n",
            "+---+---------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"SESSION\").getOrCreate()\n",
        "data_path = \"/content/DATA.csv\"\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTP-Kl_AZRTj"
      },
      "source": [
        "Source Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN0x21k4ZLzX",
        "outputId": "2c504b58-dfcf-4675-e13d-e2af4f3d3392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------+------+----------+----+\n",
            "|   Name|PostCode|noneed|       DOB| AGE|\n",
            "+-------+--------+------+----------+----+\n",
            "|   John|    6227|  8622|2004-06-30|19.0|\n",
            "|  Alice|    1251|  7236|1980-08-27|43.0|\n",
            "|    Bob|    8655|  2645|1996-02-04|27.0|\n",
            "|Charlie|    8646|  4924|1995-05-12|28.0|\n",
            "|  David|    5689|  9089|1958-05-16|65.0|\n",
            "|   Emma|    4708|  7758|1962-09-02|61.0|\n",
            "|  Frank|    8416|  4616|1976-11-25|47.0|\n",
            "|  Grace|    5812|  9282|1973-09-27|50.0|\n",
            "|   Hank|    5009|  3375|1963-01-24|60.0|\n",
            "|    Ivy|    1436|  1734|1972-06-20|51.0|\n",
            "|   Jack|    7507|  6894|1988-02-21|35.0|\n",
            "|  Katie|    8852|  2323|1997-11-24|26.0|\n",
            "|    Leo|    4244|  3231|1988-04-11|35.0|\n",
            "|    Mia|    9867|  9895|1967-04-29|56.0|\n",
            "| Nathan|    4713|  4725|1994-05-12|29.0|\n",
            "| Olivia|    2152|  5005|1993-04-15|30.0|\n",
            "|  Peter|    6097|  5286|2003-08-15|20.0|\n",
            "|  Quinn|    5612|  3838|2002-06-17|21.0|\n",
            "| Rachel|    2759|  7767|1974-09-26|49.0|\n",
            "|    Sam|    5294|  4927|1988-01-13|35.0|\n",
            "+-------+--------+------+----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_path = \"/content/source.csv\"\n",
        "sourceTable = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "sourceTable.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1U70DtlZUzZ"
      },
      "source": [
        "Target Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab3EBWIfa-ww"
      },
      "source": [
        "#Data Fetching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka4fxgaAbDHI"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWUnIprebBtu",
        "outputId": "0a1a38b1-4460-47bd-e47d-f318d8b03496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source Table:\n",
            "+-------+--------+------+----------+----+\n",
            "|   Name|PostCode|noneed|       DOB| AGE|\n",
            "+-------+--------+------+----------+----+\n",
            "|   John|    6227|  8622|2004-06-30|19.0|\n",
            "|  Alice|    1251|  7236|1980-08-27|43.0|\n",
            "|    Bob|    8655|  2645|1996-02-04|27.0|\n",
            "|Charlie|    8646|  4924|1995-05-12|28.0|\n",
            "|  David|    5689|  9089|1958-05-16|65.0|\n",
            "|   Emma|    4708|  7758|1962-09-02|61.0|\n",
            "|  Frank|    8416|  4616|1976-11-25|47.0|\n",
            "|  Grace|    5812|  9282|1973-09-27|50.0|\n",
            "|   Hank|    5009|  3375|1963-01-24|60.0|\n",
            "|    Ivy|    1436|  1734|1972-06-20|51.0|\n",
            "|   Jack|    7507|  6894|1988-02-21|35.0|\n",
            "|  Katie|    8852|  2323|1997-11-24|26.0|\n",
            "|    Leo|    4244|  3231|1988-04-11|35.0|\n",
            "|    Mia|    9867|  9895|1967-04-29|56.0|\n",
            "| Nathan|    4713|  4725|1994-05-12|29.0|\n",
            "| Olivia|    2152|  5005|1993-04-15|30.0|\n",
            "|  Peter|    6097|  5286|2003-08-15|20.0|\n",
            "|  Quinn|    5612|  3838|2002-06-17|21.0|\n",
            "| Rachel|    2759|  7767|1974-09-26|49.0|\n",
            "|    Sam|    5294|  4927|1988-01-13|35.0|\n",
            "+-------+--------+------+----------+----+\n",
            "\n",
            "\n",
            "Mapping DataFrame:\n",
            "+---+---------+----------------+\n",
            "| ID|SourceCol|       TargetCol|\n",
            "+---+---------+----------------+\n",
            "|  1|     Name|    CustomerName|\n",
            "|  2| PostCode|CustomerPostCode|\n",
            "|  3|      DOB|     CustomerDOB|\n",
            "|  4|      AGE|     CustomerAge|\n",
            "+---+---------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"sessionII\").getOrCreate()\n",
        "#SOURCE TABLE\n",
        "print(\"Source Table:\")\n",
        "sourceTable.show()\n",
        "#MAPPING DATA FRAME\n",
        "print(\"\\nMapping DataFrame:\")\n",
        "df.show()\n",
        "#QUERRY\n",
        "targetTable = sourceTable.select([col(sc).alias(tc) for sc, tc in zip(df.select(\"SourceCol\").rdd.flatMap(lambda x: x).collect(),\n",
        "                                                                       df.select(\"TargetCol\").rdd.flatMap(lambda x: x).collect())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FJAr9vsbk-Z",
        "outputId": "6ac7f0ad-ef36-4592-adb2-183fdb3380e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+----------------+-----------+-----------+\n",
            "|CustomerName|CustomerPostCode|CustomerDOB|CustomerAge|\n",
            "+------------+----------------+-----------+-----------+\n",
            "|        John|            6227| 2004-06-30|       19.0|\n",
            "|       Alice|            1251| 1980-08-27|       43.0|\n",
            "|         Bob|            8655| 1996-02-04|       27.0|\n",
            "|     Charlie|            8646| 1995-05-12|       28.0|\n",
            "|       David|            5689| 1958-05-16|       65.0|\n",
            "|        Emma|            4708| 1962-09-02|       61.0|\n",
            "|       Frank|            8416| 1976-11-25|       47.0|\n",
            "|       Grace|            5812| 1973-09-27|       50.0|\n",
            "|        Hank|            5009| 1963-01-24|       60.0|\n",
            "|         Ivy|            1436| 1972-06-20|       51.0|\n",
            "|        Jack|            7507| 1988-02-21|       35.0|\n",
            "|       Katie|            8852| 1997-11-24|       26.0|\n",
            "|         Leo|            4244| 1988-04-11|       35.0|\n",
            "|         Mia|            9867| 1967-04-29|       56.0|\n",
            "|      Nathan|            4713| 1994-05-12|       29.0|\n",
            "|      Olivia|            2152| 1993-04-15|       30.0|\n",
            "|       Peter|            6097| 2003-08-15|       20.0|\n",
            "|       Quinn|            5612| 2002-06-17|       21.0|\n",
            "|      Rachel|            2759| 1974-09-26|       49.0|\n",
            "|         Sam|            5294| 1988-01-13|       35.0|\n",
            "+------------+----------------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "targetTable.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3riN911rIG1z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEP5appqH0XT",
        "outputId": "aa627de4-2bc9-45a1-f101-ba18e8c52816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available functions:\n",
            "1. Check for Null Values\n",
            "2. Summary Statistics\n",
            "3. Unique Values Count\n",
            "4. Value Counts for Each Column\n"
          ]
        }
      ],
      "source": [
        "#Input\n",
        "print(\"Available functions:\")\n",
        "print(\"1. Check for Null Values\")\n",
        "print(\"2. Summary Statistics\")\n",
        "print(\"3. Unique Values Count\")\n",
        "print(\"4. Value Counts for Each Column\")\n",
        "\n",
        "functionSelected = input(\"Enter the number corresponding to the function you want to apply (1, 2, 3, 4, etc.): \")\n",
        "#APPLYING\n",
        "result_df = None\n",
        "if functionSelected == \"1\":\n",
        "    # NULL VALUES\n",
        "    null_counts = [F.sum(F.col(col).isNull().cast(\"int\")).alias(f\"{col}_null_count\") for col in targetTable.columns]\n",
        "    result_df = targetTable.agg(*null_counts)\n",
        "elif functionSelected == \"2\":\n",
        "    # SUMMARY STATS\n",
        "    summary_stats = [F.mean(col).alias(f\"{col}_mean\") for col in targetTable.columns] + \\\n",
        "                    [F.stddev(col).alias(f\"{col}_stddev\") for col in targetTable.columns] + \\\n",
        "                    [F.min(col).alias(f\"{col}_min\") for col in targetTable.columns] + \\\n",
        "                    [F.max(col).alias(f\"{col}_max\") for col in targetTable.columns] + \\\n",
        "                    [F.percentile_approx(col, 0.25).alias(f\"{col}_q1\") for col in targetTable.columns] + \\\n",
        "                    [F.expr(f\"percentile_approx({col}, 0.5)\").alias(f\"{col}_median\") for col in targetTable.columns] + \\\n",
        "                    [F.percentile_approx(col, 0.75).alias(f\"{col}_q3\") for col in targetTable.columns]\n",
        "    result_df = targetTable.agg(*summary_stats)\n",
        "elif functionSelected == \"3\":\n",
        "    # UNIQUE VAL COUNT\n",
        "    unique_counts = [F.countDistinct(col).alias(f\"{col}_unique_count\") for col in targetTable.columns]\n",
        "    result_df = targetTable.agg(*unique_counts)\n",
        "elif functionSelected == \"4\":\n",
        "    # ALL VAL COUNT\n",
        "    value_counts = [F.count(F.col(col)).alias(f\"{col}_count\") for col in targetTable.columns]\n",
        "    result_df = targetTable.agg(*value_counts)\n",
        "\n",
        "#RESULTS\n",
        "result_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvoUf7RJcfqR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkAQ0cirb4xT",
        "outputId": "86ad3093-2cfe-4839-d0da-e3c41b4acfd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the columns you want to select (comma-separated): CustomerAge\n",
            "Tables with selected columns: CustomerAge\n",
            "+-----------+\n",
            "|CustomerAge|\n",
            "+-----------+\n",
            "|       19.0|\n",
            "|       43.0|\n",
            "|       27.0|\n",
            "|       28.0|\n",
            "|       65.0|\n",
            "|       61.0|\n",
            "|       47.0|\n",
            "|       50.0|\n",
            "|       60.0|\n",
            "|       51.0|\n",
            "|       35.0|\n",
            "|       26.0|\n",
            "|       35.0|\n",
            "|       56.0|\n",
            "|       29.0|\n",
            "|       30.0|\n",
            "|       20.0|\n",
            "|       21.0|\n",
            "|       49.0|\n",
            "|       35.0|\n",
            "+-----------+\n",
            "\n",
            "Available functions:\n",
            "1. Sum\n",
            "2. Count\n",
            "3. Subtract\n",
            "4. Maximum\n",
            "5. Minimum\n",
            "6. Mode\n",
            "Enter the number corresponding to the function you want to apply (1, 2, 3, etc.): 5\n",
            "Result after applying the selected function:\n",
            "+-----------+\n",
            "|CustomerAge|\n",
            "+-----------+\n",
            "|       19.0|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "columnsSelected = input(\"Enter the columns you want to select (comma-separated): \")\n",
        "columnsSelected = [col.strip() for col in columnsSelected.split(',')]\n",
        "#Validation Of Columns\n",
        "invalidColumns = set(columnsSelected) - set(targetTable.columns)\n",
        "if invalidColumns:\n",
        "    print(f\"Invalid columns: {', '.join(invalidColumns)}\")\n",
        "    exit()\n",
        "#Querry To Select Columns\n",
        "selectedTable = targetTable.select(*columnsSelected)\n",
        "#Final Tables with columns\n",
        "print(f\"Tables with selected columns: {', '.join(columnsSelected)}\")\n",
        "selectedTable.show()\n",
        "\n",
        "# Input for specific function\n",
        "print(\"Available functions:\")\n",
        "print(\"1. Sum\")\n",
        "print(\"2. Count\")\n",
        "print(\"3. Subtract\")\n",
        "print(\"4. Maximum\")\n",
        "print(\"5. Minimum\")\n",
        "print(\"6. Mode\")\n",
        "\n",
        "functionSelected = input(\"Enter the number corresponding to the function you want to apply (1, 2, 3, etc.): \")\n",
        "\n",
        "# Applying\n",
        "result_df = None\n",
        "if functionSelected == \"1\":\n",
        "    result_df = selectedTable.agg(*(F.sum(col).alias(col) for col in columnsSelected))\n",
        "elif functionSelected == \"2\":\n",
        "    result_df = selectedTable.agg(*(F.count(col).alias(col) for col in columnsSelected))\n",
        "elif functionSelected == \"3\":\n",
        "    if len(columnsSelected) == 2:\n",
        "        result_df = selectedTable.select(columnsSelected[0], (selectedTable[columnsSelected[0]] - selectedTable[columnsSelected[1]]).alias(\"result\"))\n",
        "    else:\n",
        "        print(\"Subtract function requires exactly two columns.\")\n",
        "        exit()\n",
        "elif functionSelected == \"4\":\n",
        "    result_df = selectedTable.agg(*(F.max(col).alias(col) for col in columnsSelected))\n",
        "elif functionSelected == \"5\":\n",
        "    result_df = selectedTable.agg(*(F.min(col).alias(col) for col in columnsSelected))\n",
        "elif functionSelected == \"6\":\n",
        "    # Calculate mode by counting occurrences and selecting the top one\n",
        "    mode_exprs = [F.expr(f\"first({col}, true)\").alias(col) for col in columnsSelected]\n",
        "    result_df = selectedTable.groupBy().agg(*mode_exprs)\n",
        "\n",
        "\n",
        "#Resiult\n",
        "if result_df:\n",
        "    print(f\"Result after applying the selected function:\")\n",
        "    result_df.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k-0zyld9B4FR",
        "sgVJJnjwB0Wr",
        "_-GcjG0gGU5X"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}